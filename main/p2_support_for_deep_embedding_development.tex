%!TEX root = ../phd.tex
\part{Automating Deep Embedding Development}
\label{part:deep-embedding-development-with-a-breeze}



\chapter{Simplifying Transformations in the Deep Embedding}
\label{sec:yy-for-transformations}

% Typically in the deep embedding the DSL author has to perform transformations over the internal representation
%  - lowerings, simplifications, global transformations. For all of these it is necessary to construct
%    new ASTs, and deconstruct ASTs.

In the deep embedding the DSL author often performs transformations over the intermediate representation. The
transformations can be \emph{lowering} transformations that convert
domain-specific IR nodes to a lower-level representation, program optimizations, global
transformations, etc. In all of these categories the DSL author must \emph{deconstruct} ASTs
 and constructs new ASTs based on the information from deconstructed ASTs.

% In the deep embeddings there are two ways to achieve this:
 % using the naked translation. Example.
 % using the deep embedding it self. Example. Explanation.
In the deep embeddings the common ways to achieve transformations are:\begin{itemize}

\item {\bf AST manipulation.} The ``naked'' AST manipulation is typically achieved by
 using the constructors and deconstructors of the AST nodes. The disadvantage of this
 technique is that, although it gives complete control over the ASTs, it is verbose. To demonstrate
 the verbosity of the approach we show the lowering transformation of \code{Vector.fill}:\begin{lstparagraph}
 case VectorFill(size, v) =>
   VectorFromSeq(SeqFill(s, v))
 \end{lstparagraph}

\item {\bf Using the deep embedding interface.} An alternative to AST manipulation is
 the reuse of the deep embedding for transformations~\cite{rompf_optimizing_2013}. This
 approach improves the interface for the DSL authors. The lowering of \code{VectorFill}
 could be performed as:\begin{lstparagraph}
 case VectorFill(size, v) =>
   Vector.fromSeq(Seq.fill(s, v))
 \end{lstparagraph}
 This approach, however, has several drawbacks that we describe in the next section.
\end{itemize}

\section{Drawbacks of the Deep Embedding as a Transformer}
\label{sec:drawbacks-of-the-deep-embedding-as-a-transformer}

\paragraph{Non-idiomatic interface.} Using deep embedding for transformations improves over
AST manipulations but it is not ideal. The subset of the problems with the deep embedding
 interface~(\S \ref{sec:abstraction-leaks}) persists. The DSL authors are, however, more
 accustomed to the convoluted interfaces and complicated type errors.

% In LMS based DSLs lifting your code
\paragraph{Masking the standard library.} In DSLs based on polymorphic embedding there
 is a particular difficulty. When the DSL back-end uses the interface of the deep embedding
 it often masks the interface of the standard library. The DSL authors then must use full
 paths to access objects in the standard library. For example, constructing a simple hash-map
 must be achieved with \begin{lstparagraph}
collection.immutable.HashMap("Int"->typeTag[Int])
\end{lstparagraph}
because simply calling \code{HashMap("Int"->typeTag[Int])} would be captured by the deep interface
and result in producing a \code{HashMapApply} IR node.

\paragraph{Implicitly passed parameters.} Implicit parameters that were used during construction
of case classes are not implicit during their deconstruction. This makes it hard to use the same implicit
mechanisms in the deep embedding in the presence of deconstruction.

For the constructor of \code{Vector.fill} the type information can be passed as an implicit argument
and stored in the IR node:\begin{lstparagraph}
def vector_fill[T](v: Rep[T], size: Rep[Int])
  (implicit tp: TypeTag[T]) = VectorFill(v,size,tp)
\end{lstparagraph}

In the scope where the node is deconstructed, however, the parameter is not present: \begin{lstparagraph}
case VectorFill(v, size, tp) => // tp is not implicit in this scope
\end{lstparagraph}

In order to pass the type information in their transformations the users are faced with
 two choices: \emph{i)} pass the information manually, \emph{ii)} or re-introduce the implicit
 information manually in the scope. Both approaches make it harder to transform
 the ASTs with the deep embedding.

\section{Quotation for Specifying Transformations}
\label{sec:yy-as-quotation}

% We improve this by using the core translation for quotation.
To improve transformations in the deep embedding we again use the direct embedding
 as the front end. From the direct Instead of using the interface of the deep embedding we simply use the the direct
 embedding and use the translation to achieve node reification.

By using only the direct interface there are no issues with the abstraction leaks of the deep embedding.
Further, the difference between the host language terms and deep terms is clearly separated with the scope of
the translation. The lowering of \code{VectorFill} with the translation goes as follows:\begin{lstparagraph}
case VectorFill(size, v) => trans {
  Vector.fromSeq(Seq.fill(s, uq(v)))
}
\end{lstparagraph}
where the \code{trans} is the translation defined similarly to the DSL translation.


% We modify two parts of the translation: the captured variables, and scope injection.
The translation we use here has several differences compared to translation for DSL programs:\begin{itemize}
\item {Signature of the translation method.} The return type of the translation method (\code{trans} in the
example) is modified:\begin{lstparagraph}
def trans[T](body: => T): $\tau$(T)
\end{lstparagraph} The return type is now the type in the deep embedding ($\tau$ is the type translation for a given DSL).

\item {Captured variables.} The captured variables (\code{s} and \code{v} in the example)
 have incompatible types with the direct embedding. We tackle this problem by allowing
 \emph{unquoting} of captured variables with the function \code{uq} that has a signature:\begin{itemize}
  def uq[T](t: $\tau$(T)): T
\end{itemize}
An implicit version of this function is also provided that allows the user to omit the function in many cases.

In the example, the value \code{v} must use the \code{uq} function as
the function \code{fill} is generic and passing a \code{$\tau$(t)} would pass the type checking and never
introduce the implicit version of the function. The captured variable \code{s}, on the other hand,
has the expected type \code{Int}. The wrong expected type will trigger the implicit version of \code{uq}
that unquotes the term.

\item {Scope injection.} For AST transformation the scope injection phase is defined differently. In case of the
 polymorphic embeddings, where all DSL definitions are inside Scala components, the scope injection phase is not necessary. The
 translated code is already in the scope of the deep embedding definitions.

\end{itemize}

\subsection{Passing Implicit Parameters}
\label{sec:dealing-with-implict-parameters}

To minimize the work required to deal with implicit arguments we modify the transformation
method introduce them automatically. First we allow the method to accept a list of
 nodes whose parameters with type information should be implicit in the scope:\begin{lstparagraph}
def trans[T](nodes: Any*)(body: => T): $\tau$(T)
\end{lstparagraph}

To allow introduction of implicits we modify the scope injection phase to introduce the
implicits required by the DSL author. The way how implicits are extracted from the node
are left to the DSL author. In the following example we will find all the \code{TypeTag}s
in the signature of the node and provide them in scope implicitly. With this scheme
the previous example becomes:\begin{lstparagraph}
case node@VectorFill(size, v, _) => trans(node) {
  Vector.fromSeq(Seq.fill(s, uq(v)))
}
\end{lstparagraph}
and it expands to\begin{lstparagraph}
case node @ VectorFill(size, v, _) =>
  class T // A synthetic class
  implicit val impl$\$$0 = node.tpe.asInstanceOf[TypeTag[T]]
  val v$\$$1 = v.asInstanceOf[$\tau$(T)]

  lift[Vector].fromSeq(lift[Seq].fill(s, v$\$$1))
}
\end{lstparagraph}


\section{AST Deconstruction}
\label{sec:yy-ast-deconstruction}

A similar mechanism as with node construction could be used for AST deconstruction. In this case the
 deep embedding domain must be changed to perform deconstruction instead of construction of IR nodes.
 The deconstruction of nodes can be achieved by using Scala quasi-quotation in pattern matching. Deconstruction
 of \code{VectorFill}, from the example, would be achieved with:\begin{lstparagraph}
 case ext"Vector.fill($\$$size, $\$$v)" =>
   // here variables `size' and `v' have expected types
 \end{lstparagraph}


 In order to achieve this we must provide an alternative deep embedding for
  node deconstruction with the interface prescribed by the core translation. We
  leave this for future work.





\chapter{Generation of the Deep Embedding}
\label{sec:deep-gen}

So far, we have seen how \yy translates programs written in the
direct embedding to the deep embedding. This arguably simplifies life
for \edsl users by allowing them to work with the interface of the
direct embedding.  However, the EDSL author still needs to maintain synchronized
 implementations of the two embeddings, which can be a tedious and error prone task.

% Two steps
To alleviate this issue, \yy automatically generates the deep embedding from
the implementation of the direct embedding. This happens in two steps: First, we
generate high-level IR nodes and methods that construct them through a
systematic conversion of methods declared in a direct embedding to their
corresponding methods in the deep embedding~(\S \ref{sec:yy-impl-def}).
%
Second, we exploit the fact that method implementations in the direct
embedding are also direct DSL programs.  Reusing our core translation
from~\S \ref{sec:translation}, we translate them to their deep
counterparts~(\S \ref{sec:yy-impl-lower}).

%% sstucki: what exactly does this mean?
%% \todo{Explain that there is a core language.}

% In the translated method bodies, in addition to the translated DSL itself, we also allow
% usage of the Scala library constructs that supported by the target back-end (cf.~\cite{techrep}).

The automatic generation of deep embeddings reduces the amount of
boilerplate code that has to be written and maintained by \edsl
authors, allowing them to instead focus on tasks that can not be
easily automated, such as the implementation of domain-specific
optimizations in the deep embedding.  However, automatic code
generation is not a silver bullet.  Hand-written optimizations acting
on the IR typically depend on the structure of the later, introducing
hidden dependencies between such optimizations and the direct
embedding.  Care must be taken in order to avoid breaking
optimizations when changing the direct embedding of the \edsl.

 While these optimizations are not re-generated; only the
 components that correspond to the interface and the IR nodes are
 modified. Therefore, the DSL author is only responsible for
 maintaining analysis and optimizations in the deep embedding. A change
 in the direct embedding interface should affect only optimizations
 related to that change. However, this is back-end dependent so \yy
 does not provide any guarantees.

%% \todo{Give an example here.}

\section{Constructing High-Level IR Nodes}
\label{sec:yy-impl-def}

To make the generation regular \yy provides a corresponding IR node and
construction method for every operation in the direct embedding. By using
reflection, we extract the method signatures from the direct embedding. From
these, we generate the interface, implementation, and code generation traits as
prescribed by LMS. This part of the translation is LMS specific and applying it
to other frameworks would require changing the code templates. Based on the
signature of each method, we generate the \emph{case class} that represents the
IR node. Then, for each method we generate a corresponding method that
instantiates the high-level IR nodes. Whenever a method is invoked in the deep
EDSL, instead of being evaluated, a high-level IR node is created.

\figref{lst:vector_deep_ir} illustrates the way of defining IR nodes for
\code{Vector} EDSL. The case classes in the \code{VectorOps} trait define the
IR nodes for each method in the direct embedding. The fields of these case
classes are the callee object of the corresponding method (e.g., \code{v} in
\code{VectorMap}), and the arguments of that method (e.g., \code{f} in
\code{VectorMap}).

\begin{figure}
\begin{listingtiny}
trait VectorOps extends SeqOps with
  NumericOps with Base {
  // elided implicit enrichment methods. E.g.:
  //   Vector.fill(v, n) = vector_fill(v, n)

  // High level IR node definitions
  case class VectorMap[T:Numeric,S:Numeric]
    (v: Rep[Vector[T]], f: Rep[T] => Rep[S])
    extends Rep[Vector[S]]
  case class VectorFill[T:Numeric]
    (v: Rep[T], size: Rep[Int])
    extends Rep[Vector[T]]

  def vector_map[T:Numeric,S:Numeric]
    (v: Rep[Vector[T]], f: Rep[T] => Rep[S]) =
      VectorMap(v, f)
  def vector_fill[T:Numeric]
    (v: Rep[T], size: Rep[Int]) =
    VectorFill(v, size)
}
\end{listingtiny}
\caption{\label{lst:vector_deep_ir} High-level IR nodes for Vector.}
\end{figure}

% Side-effects

Deep embedding should, in certain cases, be aware of side-effects. The EDSL
author must annotate methods that cause side-effects with an appropriate
annotation. To minimize the number of needed annotations we use Scala FX
\cite{rytz2012lightweight}. Scala FX is a compiler plugin that adds an effect
system on top of the Scala type system. With Scala FX the regular Scala type
inference also infers the effects of expressions. As a result, if the direct
\edsl is using libraries which are already annotated, like the Scala collection
library, then the EDSL author does not have to annotate the direct EDSL.
Otherwise, there is a need for manual annotation of the direct embedding by
the EDSL author. Finally, the Scala FX annotations are mapped to the
corresponding effect construct in LMS.

\figref{lst:vector_deep_fx} shows how we automatically transform the I/O
effect of a \code{print} method to the appropriate construct in LMS. As the
Scala FX plugin knows the effect of \code{System.out.println}, the effect for
the \code{print} method is inferred together with its result type
(\code{Unit}). Based on the fact that the \code{print} method has an I/O
effect, we wrap the high-level IR node creation method into
\code{reflect}, which is an effect construct in LMS to specify an I/O
effect~\cite{rompf_building-blocks_2011}. In effect, all optimizations in the \edsl
will have to preserve the order of \code{println} and other I/O effects. We omit details
about the LMS effect system; for more details cf. \cite{rompf_building-blocks_2011}.

\begin{figure}
\begin{listingtiny}
class Vector[T: Numeric](val data: Seq[T]) {
  // effect annotations not necessary
  def print() = System.out.print(data)
}
trait VectorOps extends SeqOps with
  NumericOps with Base {
  case class VectorPrint[T:Numeric]
    (v: Rep[Vector[T]]) extends Rep[Vector[T]]
  def vector_print[T:Numeric](v: Rep[Vector[T]]) =
    reflect(VectorPrint(v))
}
\end{listingtiny}
\caption{\label{lst:vector_deep_fx} Direct and deep embedding for Vector with side-effects.}
\end{figure}

\subsection{Lowering High-Level IR Nodes to Their Low-Level Implementation}
\label{sec:yy-impl-lower}

Having domain-specific optimizations on the high-level representation is not
enough for generating high performance code. In order to improve the
performance, we must transform these high-level nodes into their corresponding
low-level implementations. Hence, we must represent the low-level
implementation of each method in the deep EDSL. After creating the high-level
IR nodes and applying domain-specific optimizations, we transform these IR
nodes into their corresponding low-level implementation. This can be achieved by
using a \emph{lowering} phase \cite{rompf_optimizing_2013}.

\figref{lst:vector_deep_low} illustrates how the invocation of each method
results in creating an IR node together with a lowering specification for
transforming it into its low-level implementation. For example, whenever the
method \code{fill} is invoked, a \code{VectorFill} IR node is created like
before. However, this high-level IR node needs to be transformed to its low-level
IR nodes in the lowering phase. This delayed transformation is specified
using an \code{atPhase(lowering)} block~\cite{rompf_optimizing_2013}.
Furthermore, the low-level implementation uses constructs requiring deep
embedding of other interfaces. In particular, an implementation of the
\code{fill} method requires the \code{Seq.fill} method that is provided by
the \code{SeqLowLevel} trait.
\begin{figure}[ht]
\begin{listingtiny}
trait VectorLowLevel extends VectorOps
  with SeqLowLevel {
  // Low level implementations
  override def vector_fill[T:Numeric]
    (v: Rep[T], s: Rep[Int]) =
    VectorFill(v, s) atPhase(lowering) {
      Vector.fromSeq(Seq.fill[T](s)(v))
    }
}
\end{listingtiny}
\caption{\label{lst:vector_deep_low} Lowering to the low-level implementation for Vector.}
\end{figure}

Generating the low-level implementation is achieved by transforming the
implementation of each direct embedding method. This is done in two steps.
First, the expression given as the implementation of a method is converted to
a Scala AST of the deep embedding by core translation of \yy. Second, the
code represented by the Scala AST must be injected back to the corresponding
trait. To this effect, we implemented Sprinter~\cite{sprinter}, a library that generates
correct and human readable code out of Scala ASTs. The generated source code
is used to represent the lowering specification of every IR node.

\section{Evaluation}
\label{sec:eval-deepgen}

To evaluate the automatic deep EDSL generation for OptiML, OptiQL, and Vector,
we used Forge~\cite{forge}, a Scala based meta-EDSL for generating both direct
and deep EDSLs from a single specification. Forge already contained
specifications for OptiML and OptiQL.

To avoid re-typing OptiML and OptiQL we modified Forge to generate the direct
embedding from its specification and generated the direct embeddings
from the existing Forge based \edsl specifications. Then, we used our automatic deep
generation tool to convert these direct embeddings into their deep
counterparts. Since, deep \edsls mostly consist of boilerplate the generated
embeddings have a similar number of LOC as the handwritten counterparts. For all
three \edsls, we verified that tests running in the direct embeddings behave the
same as the tests for the deep embeddings.

In Table \ref{tbl:deepgen}, we give a line count comparison for the code in
the direct embedding, Forge specification, and deep embedding for three \edsls:
\emph{i) OptiML} is a Delite-based \edsl for machine learning,
\emph{ii) OptiQL} is a Delite-based \edsl for running in-memory queries, and
\emph{iii) Vector} is the EDSL shown as an example throughout this paper.
We are careful with measuring lines-of-code (LOC) with Forge and the deep EDSLs: we only
count the parts which are generated out of the given direct EDSL.

Overall, \yy requires roughly the same number of LOC as Forge to specify the DSL.
This can be viewed as positive result since Forge relies on a specific meta-language
 for defining the two embeddings. \yy, however, uses Scala itself
for this purpose and is thus much easier to use. In case of OptiML, Forge
slightly outperforms \yy. This is because Forge supports meta-programming at
the level of classes while Scala does not.

\begin{table}[ht]
\caption{LOC for direct EDSL, Forge specification, and deep EDSL.}
\label{tbl:deepgen}
\centering
\begin{tabularx}{\linewidth}{ X X X X }
\toprule
 EDSL       &   Direct      &     Forge     &   Deep      \\ \midrule
OptiML      &   1128        &     1090    &   5876      \\
OptiQL      &   73          &     74      &   526       \\
Vector      &   70          &     71      &   369       \\
\bottomrule
\end{tabularx}
\end{table}

We did not compare the efforts required to specify the DSL with \yy and Forge. The
reason is twofold:
\begin{itemize}

\item It is hard to estimate the effort required to design a DSL. If the same
person designs a single DSL twice, the second implementation will always be
easier and take less time. On the other hand, when multiple people implement a DSL their skill levels
can greatly differ. Finally, DSL design is technically demanding and it is hard to find
a large enough group to conduct a statistically significant user
study.

\item Writing the direct embedding in Scala is arguably simpler than writing a
Forge specification. Forge is Delite-specific language and uses a custom
preprocessor to define method bodies in Scala. Thus, learning a new language and
combining it with Scala snippets must be harder than just writing idiomatic
Scala.
\end{itemize}


\chapter{Automatic Management of Dynamic Compilation of DSLs \todo{Not Finished}}
\label{sec:dynamic-compilation}

Domain-specific language (DSL) compilers often require knowledge of values known only
 at program run-time to perform optimizations. For example,
 in matrix-chain multiplication, knowing matrix sizes allows choosing the
 optimal multiplication order~\cite[Ch.~15.2]{cormen2001introduction} and
 in relational algebra knowing relation sizes is necessary for choosing
 the fastest order of join operators~\cite{selinger1979access}. Consider the example
 of matrix-chain multiplication:\begin{lstparagraph}
  val (m1, m2, m3) = ... // matrices of unknown size
  m1 * m2 * m3
\end{lstparagraph}

In this program, without knowing the matrix sizes, the DSL compiler can not determine the
 optimal order of multiplications. There are two possible orders
 \code{(m1*m2)*m3} with an estimated cost \code{c1} and \code{m1*(m2*m3)} with an estimated cost \code{c2} where:\begin{lstparagraph}
  c1 = m1.rows*m1.columns*m2.columns+m1.rows*m2.columns*m3.rows
  c2 = m2.rows*m2.columns*m3.columns+m1.rows*m2.rows*m3.columns
\end{lstparagraph}

Ideally we would change the multiplication order at run-time only when the condition
 \code{c1 > c2} changes and not the individual inputs. Matrix-chain multiplication
 requires global transformations based on conditions outside the program scope.
 For this task \emph{dynamic compilation}~\cite{auslander1996fast} seems as a good fit.

Yet, dynamic compilation systems---such as DyC~\cite{grant2000dyc} and JIT compilers---have shortcomings.
 They use run-time information primarily for program specialization. In these systems,
 values are profiled for \emph{stability}, i.e., having the same value over multiple executions.
 Then, \emph{recompilation guards} and \emph{code caches} are based on \emph{checking equality}
 of current values and previously stable values.

 To perform domain-specific optimizations, however, we must check stability, introduce recompilation guards,
  and code caches, based on the computation specified in the DSL optimizer---outside the user program.
  Ideally, the DSL optimizer should be agnostic of the fact that input values
  are collected at runtime. In the example stability is only required for the condition \code{c1 > c2}, while the values
  \code{c1} and \code{c2} themselves are allowed to be unstable. Finally, recompilation guards
  and code caches would recompile and reclaim code based on the same condition.
% TODO this is not well described

An exception to existing dynamic compilation systems is Truffle~\cite{wurthinger2013one}.
 Truffle allows creation of user defined recompilation guards based on
 DSL author defined computations. However, with Truffle, language designers do not have the full view of the program,
 and thus, can not perform global optimizations (e.g., matrix-chain multiplication optimization).
 Further, recompilation guards must be manually introduced and the code in the DSL optimizer must
 be modified to specially handle decisions based on runtime values.

We propose a dynamic compilation system aimed for domain-specific languages where:
\begin{itemize}

  \item DSL authors declaratively, at the definition site, state the values that
    are of interest for dynamic compilation (e.g., array and matrix sizes, vector
    and matrix sparsity). These values can regularly be used for making
    compilation decisions throughout the DSL compilation pipeline.

  \item The instrumented DSL compiler transparently reifies all computations on
   the runtime values that affect compilation decisions. In our example,
   the compiler reifies and stores all computations on run-time values in the
   unmodified dynamic programming algorithm~\cite{cormen2001introduction} for determining the
   optimal multiplication order (i.e., \code{c1 > c2}).

  \item Recompilation guards are introduced automatically based on the reified decisions made during DSL
   compilation. In the example the recompilation guard would be \code{c1 > c2}.

  \item Code caches are automatically addressed with outcomes of DSL compilation
   decisions instead of stable values from user programs. In the example
   the code cache would have two entries addressed with a single boolean
   value computed with \code{c1 > c2}.

\end{itemize}

We evaluate \ref{sec:p2-evaluation} on matrix-chain multiplication algorithm described
 in \ref{sec:matrix-chain-multiplication}. We show that the DSL user needs to add only
 4 annotations in the original algorithm to enable automatic dynamic compilation. We further
 show that automatically introduced compilation guards reduce compilation overheads by orders of magnitude
 compared to classical dynamic compilation.

\section{Abstractions for Managing Dynamic Compilation}

\subsection{Reifying the Compilation Process}

\section{Case Study: Matrix-Chain Multiplication}
\label{sec:matrix-chain-multiplication}

\section{Evaluation}
\label{sec:p2-evaluation}

