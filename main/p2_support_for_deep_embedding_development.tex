%!TEX root = ../phd.tex
\part{Automating Deep Embedding Development}
\label{part:deep-embedding-development-with-a-breeze}

\chapter{\yy Translation for Transformations in the Deep Embedding}
\label{sec:yy-for-transformations}



\chapter{Generation of the Deep Embedding}
\label{sec:deep-gen}

So far, we have seen how \yy translates programs written in the
direct embedding to the deep embedding. This arguably simplifies life
for \edsl users by allowing them to work with the interface of the
direct embedding.  However, the EDSL author still needs to maintain synchronized
 implementations of the two embeddings, which can be a tedious and error prone task.

% Two steps
To alleviate this issue, \yy automatically generates the deep embedding from
the implementation of the direct embedding. This happens in two steps: First, we
generate high-level IR nodes and methods that construct them through a
systematic conversion of methods declared in a direct embedding to their
corresponding methods in the deep embedding~(\S \ref{sec:yy-impl-def}).
%
Second, we exploit the fact that method implementations in the direct
embedding are also direct DSL programs.  Reusing our core translation
from~\S \ref{sec:translation}, we translate them to their deep
counterparts~(\S \ref{sec:yy-impl-lower}).

%% sstucki: what exactly does this mean?
\todo{Explain that there is a core language.}

% In the translated method bodies, in addition to the translated DSL itself, we also allow
% usage of the Scala library constructs that supported by the target back-end (cf.~\cite{techrep}).

The automatic generation of deep embeddings reduces the amount of
boilerplate code that has to be written and maintained by \edsl
authors, allowing them to instead focus on tasks that can not be
easily automated, such as the implementation of domain-specific
optimizations in the deep embedding.  However, automatic code
generation is not a silver bullet.  Hand-written optimizations acting
on the IR typically depend on the structure of the later, introducing
hidden dependencies between such optimizations and the direct
embedding.  Care must be taken in order to avoid breaking
optimizations when changing the direct embedding of the \edsl.

 While these optimizations are not re-generated; only the
 components that correspond to the interface and the IR nodes are
 modified. Therefore, the DSL author is only responsible for
 maintaining analysis and optimizations in the deep embedding. A change
 in the direct embedding interface should affect only optimizations
 related to that change. However, this is back-end dependent so \yy
 does not provide any guarantees.

\todo{Give an example here.}

\section{Constructing High-Level IR Nodes}
\label{sec:yy-impl-def}

To make the generation regular \yy provides a corresponding IR node and
construction method for every operation in the direct embedding. By using
reflection, we extract the method signatures from the direct embedding. From
these, we generate the interface, implementation, and code generation traits as
prescribed by LMS. This part of the translation is LMS specific and applying it
to other frameworks would require changing the code templates. Based on the
signature of each method, we generate the \emph{case class} that represents the
IR node. Then, for each method we generate a corresponding method that
instantiates the high-level IR nodes. Whenever a method is invoked in the deep
EDSL, instead of being evaluated, a high-level IR node is created.

\figref{lst:vector_deep_ir} illustrates the way of defining IR nodes for
\code{Vector} EDSL. The case classes in the \code{VectorOps} trait define the
IR nodes for each method in the direct embedding. The fields of these case
classes are the callee object of the corresponding method (e.g., \code{v} in
\code{VectorMap}), and the arguments of that method (e.g., \code{f} in
\code{VectorMap}).

\begin{figure}
\begin{listingtiny}
trait VectorOps extends SeqOps with
  NumericOps with Base {
  // elided implicit enrichment methods. E.g.:
  //   Vector.fill(v, n) = vector_fill(v, n)

  // High level IR node definitions
  case class VectorMap[T:Numeric,S:Numeric]
    (v: Rep[Vector[T]], f: Rep[T] => Rep[S])
    extends Rep[Vector[S]]
  case class VectorFill[T:Numeric]
    (v: Rep[T], size: Rep[Int])
    extends Rep[Vector[T]]

  def vector_map[T:Numeric,S:Numeric]
    (v: Rep[Vector[T]], f: Rep[T] => Rep[S]) =
      VectorMap(v, f)
  def vector_fill[T:Numeric]
    (v: Rep[T], size: Rep[Int]) =
    VectorFill(v, size)
}
\end{listingtiny}
\caption{\label{lst:vector_deep_ir} High-level IR nodes for Vector.}
\end{figure}

% Side-effects

Deep embedding should, in certain cases, be aware of side-effects. The EDSL
author must annotate methods that cause side-effects with an appropriate
annotation. To minimize the number of needed annotations we use Scala FX
\cite{rytz2012lightweight}. Scala FX is a compiler plugin that adds an effect
system on top of the Scala type system. With Scala FX the regular Scala type
inference also infers the effects of expressions. As a result, if the direct
\edsl is using libraries which are already annotated, like the Scala collection
library, then the EDSL author does not have to annotate the direct EDSL.
Otherwise, there is a need for manual annotation of the direct embedding by
the EDSL author. Finally, the Scala FX annotations are mapped to the
corresponding effect construct in LMS.

\figref{lst:vector_deep_fx} shows how we automatically transform the I/O
effect of a \code{print} method to the appropriate construct in LMS. As the
Scala FX plugin knows the effect of \code{System.out.println}, the effect for
the \code{print} method is inferred together with its result type
(\code{Unit}). Based on the fact that the \code{print} method has an I/O
effect, we wrap the high-level IR node creation method into
\code{reflect}, which is an effect construct in LMS to specify an I/O
effect~\cite{rompf_building-blocks_2011}. In effect, all optimizations in the \edsl
will have to preserve the order of \code{println} and other I/O effects. We omit details
about the LMS effect system; for more details cf. \cite{rompf_building-blocks_2011}.

\begin{figure}
\begin{listingtiny}
class Vector[T: Numeric](val data: Seq[T]) {
  // effect annotations not necessary
  def print() = System.out.print(data)
}
trait VectorOps extends SeqOps with
  NumericOps with Base {
  case class VectorPrint[T:Numeric]
    (v: Rep[Vector[T]]) extends Rep[Vector[T]]
  def vector_print[T:Numeric](v: Rep[Vector[T]]) =
    reflect(VectorPrint(v))
}
\end{listingtiny}
\caption{\label{lst:vector_deep_fx} Direct and deep embedding for Vector with side-effects.}
\end{figure}

\subsection{Lowering High-Level IR Nodes to Their Low-Level Implementation}
\label{sec:yy-impl-lower}

Having domain-specific optimizations on the high-level representation is not
enough for generating high performance code. In order to improve the
performance, we must transform these high-level nodes into their corresponding
low-level implementations. Hence, we must represent the low-level
implementation of each method in the deep EDSL. After creating the high-level
IR nodes and applying domain-specific optimizations, we transform these IR
nodes into their corresponding low-level implementation. This can be achieved by
using a \emph{lowering} phase \cite{rompf_optimizing_2013}.

\figref{lst:vector_deep_low} illustrates how the invocation of each method
results in creating an IR node together with a lowering specification for
transforming it into its low-level implementation. For example, whenever the
method \code{fill} is invoked, a \code{VectorFill} IR node is created like
before. However, this high-level IR node needs to be transformed to its low-level
IR nodes in the lowering phase. This delayed transformation is specified
using an \code{atPhase(lowering)} block~\cite{rompf_optimizing_2013}.
Furthermore, the low-level implementation uses constructs requiring deep
embedding of other interfaces. In particular, an implementation of the
\code{fill} method requires the \code{Seq.fill} method that is provided by
the \code{SeqLowLevel} trait.
\begin{figure}[ht]
\begin{listingtiny}
trait VectorLowLevel extends VectorOps
  with SeqLowLevel {
  // Low level implementations
  override def vector_fill[T:Numeric]
    (v: Rep[T], s: Rep[Int]) =
    VectorFill(v, s) atPhase(lowering) {
      Vector.fromSeq(Seq.fill[T](s)(v))
    }
}
\end{listingtiny}
\caption{\label{lst:vector_deep_low} Lowering to the low-level implementation for Vector.}
\end{figure}

Generating the low-level implementation is achieved by transforming the
implementation of each direct embedding method. This is done in two steps.
First, the expression given as the implementation of a method is converted to
a Scala AST of the deep embedding by core translation of \yy. Second, the
code represented by the Scala AST must be injected back to the corresponding
trait. To this effect, we implemented Sprinter~\cite{sprinter}, a library that generates
correct and human readable code out of Scala ASTs. The generated source code
is used to represent the lowering specification of every IR node.

\section{Evaluation}
\label{sec:eval-deepgen}

To evaluate the automatic deep EDSL generation for OptiML, OptiQL, and Vector,
we used Forge~\cite{forge}, a Scala based meta-EDSL for generating both direct
and deep EDSLs from a single specification. Forge already contained
specifications for OptiML and OptiQL.

To avoid re-typing OptiML and OptiQL we modified Forge to generate the direct
embedding from its specification and generated the direct embeddings
from the existing Forge based \edsl specifications. Then, we used our automatic deep
generation tool to convert these direct embeddings into their deep
counterparts. Since, deep \edsls mostly consist of boilerplate the generated
embeddings have a similar number of LOC as the handwritten counterparts. For all
three \edsls, we verified that tests running in the direct embeddings behave the
same as the tests for the deep embeddings.

In Table \ref{tbl:deepgen}, we give a line count comparison for the code in
the direct embedding, Forge specification, and deep embedding for three \edsls:
\emph{i) OptiML} is a Delite-based \edsl for machine learning,
\emph{ii) OptiQL} is a Delite-based \edsl for running in-memory queries, and
\emph{iii) Vector} is the EDSL shown as an example throughout this paper.
We are careful with measuring lines-of-code (LOC) with Forge and the deep EDSLs: we only
count the parts which are generated out of the given direct EDSL.

Overall, \yy requires roughly the same number of LOC as Forge to specify the DSL.
This can be viewed as positive result since Forge relies on a specific meta-language
 for defining the two embeddings. \yy, however, uses Scala itself
for this purpose and is thus much easier to use. In case of OptiML, Forge
slightly outperforms \yy. This is because Forge supports meta-programming at
the level of classes while Scala does not.

\begin{table}[ht]
\caption{LOC for direct EDSL, Forge specification, and deep EDSL.}
\label{tbl:deepgen}
\centering
\begin{tabularx}{\linewidth}{ X X X X }
\toprule
 EDSL       &   Direct      &     Forge     &   Deep      \\ \midrule
OptiML      &   1128        &     1090    &   5876      \\
OptiQL      &   73          &     74      &   526       \\
Vector      &   70          &     71      &   369       \\
\bottomrule
\end{tabularx}
\end{table}

We did not compare the efforts required to specify the DSL with \yy and Forge. The
reason is twofold:
\begin{itemize}

\item It is hard to estimate the effort required to design a DSL. If the same
person designs a single DSL twice, the second implementation will always be
easier and take less time. On the other hand, when multiple people implement a DSL their skill levels
can greatly differ. Finally, DSL design is technically demanding and it is hard to find
a large enough group to conduct a statistically significant user
study.

\item Writing the direct embedding in Scala is arguably simpler than writing a
Forge specification. Forge is Delite-specific language and uses a custom
preprocessor to define method bodies in Scala. Thus, learning a new language and
combining it with Scala snippets must be harder than just writing idiomatic
Scala.
\end{itemize}


\chapter{Dynamic Compilation of Domain-Specific Languages}
\label{sec:dynamic-compilation}


